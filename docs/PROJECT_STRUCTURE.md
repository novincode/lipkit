# LipKit Project Structure

```
lipsync-blender/
â”œâ”€â”€ README.md                          # Main documentation
â”œâ”€â”€ LICENSE                            # MIT License
â”œâ”€â”€ QUICKSTART.md                      # Quick start guide
â”œâ”€â”€ .gitignore                         # Git ignore rules
â”œâ”€â”€ AI_PROMPT.txt                      # Original project requirements
â”‚
â”œâ”€â”€ lipkit/                            # Main extension folder
â”‚   â”œâ”€â”€ __init__.py                    # Extension entry point & registration
â”‚   â”œâ”€â”€ blender_manifest.toml          # Blender 4.2+ extension metadata
â”‚   â”œâ”€â”€ preferences.py                 # Addon preferences (API keys, paths)
â”‚   â”œâ”€â”€ properties.py                  # Scene property groups
â”‚   â”œâ”€â”€ operators.py                   # Blender operators
â”‚   â”œâ”€â”€ ui.py                          # UI panels
â”‚   â”‚
â”‚   â”œâ”€â”€ core/                          # Core logic (Blender-independent)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ phoneme_engine.py          # Data structures & provider base
â”‚   â”‚   â”œâ”€â”€ controller.py              # Single-property controller system
â”‚   â”‚   â”œâ”€â”€ mapping.py                 # Phoneme-to-visual mapping
â”‚   â”‚   â””â”€â”€ animation_engine.py        # Keyframe & driver generation
â”‚   â”‚
â”‚   â”œâ”€â”€ phoneme_providers/             # Phoneme extraction implementations
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ local_provider.py          # Local tools (Rhubarb, Allosaurus)
â”‚   â”‚   â””â”€â”€ api_provider.py            # Cloud API & custom endpoints
â”‚   â”‚
â”‚   â”œâ”€â”€ visual_systems/                # Visual target handlers
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ visual_system.py           # GP layers, shape keys, etc.
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/                         # Utilities
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ audio_utils.py             # Audio loading, VSE, caching
â”‚   â”‚
â”‚   â””â”€â”€ presets/                       # Phoneme presets (JSON)
â”‚       â”œâ”€â”€ preston_blair.json         # Classic 9-shape preset
â”‚       â””â”€â”€ arpabet.json               # Full English phoneme set
â”‚
â””â”€â”€ docs/                              # Documentation
    â”œâ”€â”€ ARCHITECTURE.md                # Technical design & internals
    â”œâ”€â”€ API.md                         # Python API reference
    â””â”€â”€ DEVELOPMENT.md                 # Contributing & extending guide
```

## File Count Summary

- **Python files**: 13
- **JSON presets**: 2
- **Documentation**: 6 files
- **Configuration**: 2 files (.gitignore, blender_manifest.toml)

**Total**: 23 files organized in 7 directories

## Key Design Decisions

### 1. **Modular Architecture**
- `core/` contains all business logic (testable, reusable)
- `phoneme_providers/` are pluggable (easy to add new providers)
- `visual_systems/` are extensible (support new target types)

### 2. **Single Controller Pattern**
- One `Empty` object with one custom property: `phoneme_index`
- All mouth shapes controlled via drivers reading this property
- **Result**: Clean timeline with only one animated channel!

### 3. **Headless-First Design**
- Core logic has no Blender UI dependencies
- Can be used programmatically via Python API
- UI is thin wrapper around core functionality

### 4. **Preset System**
- Mappings stored as JSON files
- Easy to share and version control
- Users can create custom presets

### 5. **Cache System**
- Phoneme extraction results cached in temp dir
- Hash-based keys (audio content + language + provider)
- Speeds up iteration during animation refinement

## Quick Stats

### Lines of Code (approximate)
- Core engine: ~800 lines
- Phoneme providers: ~400 lines
- Visual systems: ~300 lines
- UI & operators: ~600 lines
- Utilities: ~200 lines
- **Total**: ~2,300 lines of Python

### Features Implemented
âœ… Local phoneme extraction (with mock data fallback)
âœ… Cloud API provider (ready for backend)
âœ… Custom API provider
âœ… Grease Pencil layer system
âœ… Shape key system
âœ… Controller creation & management
âœ… Driver generation
âœ… NLA strip support
âœ… Audio from file or VSE
âœ… Preset system (2 presets included)
âœ… Caching system
âœ… Auto-mapping based on names
âœ… Comprehensive UI panels
âœ… Error handling & validation
âœ… Preferences for API keys & paths

### Features Planned (TODO in code)
- Image sequence system (texture switching)
- Geometry Nodes system
- Real-time preview
- Batch processing
- Whisper integration
- Better audio duration detection

## How to Use This Structure

### For Users
1. Copy `lipkit/` folder to Blender extensions directory
2. Enable in Blender preferences
3. Follow QUICKSTART.md

### For Developers
1. Read ARCHITECTURE.md to understand design
2. Read DEVELOPMENT.md for contribution guidelines
3. Use API.md for Python scripting reference
4. Core code in `core/` is Blender-independent (easier to test)

### For Contributors
- New phoneme providers â†’ `phoneme_providers/`
- New visual systems â†’ `visual_systems/`
- New presets â†’ `presets/` (JSON files)
- Documentation â†’ `docs/`

## Dependencies

### Required (Built-in to Blender)
- bpy (Blender Python API)
- Python 3.10+ standard library

### Optional (for providers)
- `requests` - for API providers (may need to install in Blender's Python)
- External tools:
  - Rhubarb Lip Sync (local provider)
  - Allosaurus (local provider)

### No External Dependencies for Core
The core engine works with mock data, so users can test without installing tools!

## Testing the Installation

Run this in Blender's Python console to verify:

```python
import lipkit
from lipkit.core import LipSyncController, PhonemeMapping
from lipkit.phoneme_providers import LocalPhonemeProvider

print("âœ… LipKit imported successfully!")
print(f"Version: {lipkit.bl_info['version']}")

# Test controller creation
controller = LipSyncController.create_controller()
print(f"âœ… Controller created: {controller.name}")

# Test provider (uses mock data)
provider = LocalPhonemeProvider()
print(f"âœ… Provider available: {provider.is_available()}")
```

## Next Steps After Installation

1. **Test with mock data**: Follow QUICKSTART.md to generate lip sync
2. **Install Rhubarb**: Download from GitHub for real phoneme extraction
3. **Configure API**: Get API key from lipkit.dev (coming soon)
4. **Create custom presets**: Copy JSON preset and modify for your character
5. **Explore Python API**: Use `quick_generate()` function for scripting

## Support & Community

- ğŸ“– Read docs/ for detailed information
- ğŸ› Report issues on GitHub
- ğŸ’¬ Ask questions in Discussions
- ğŸ¤ Contribute via pull requests

---

**Built with â¤ï¸ for the Blender animation community**
